# app.py
import streamlit as st
import os
import time
import uuid
from rag_logic import (
    prepare_vectorstore,        # ë²¡í„°ìŠ¤í† ì–´ ì¤€ë¹„(ì €ì¥/ë¡œë“œ, í•´ì‹œ ë¹„êµ)
    build_retriever,            # retriever ìƒì„±
    build_streaming_lìš”",
        "ì •ë³´ê³µê°œë¥¼ ì²­êµ¬í•˜ê³  ì‹¶ì–´ìš”",
        "ê±´ì¶•í—ˆê°€ ì‹ ì²­ì„ í•˜ê³  ì‹¶ì–´ìš”"
    ]
    for q in quick_qs:
        if st.sidebar.button(q, key=f"btn_{q}"):
            if not st.session_state.processing and st.session_state.last_clicked_question != q:
                st.session_state.selected_question = q
                st.session_state.last_clicked_question = q

    st.sidebar.markdown("---")
    if st.sidebar.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.messages.clear()
        st.session_state.question_count = 0
        st.session_state.selected_question = None
        st.session_state.last_clicked_question = None
        st.experimental_rerun()


# ===== 4. ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ë²¡í„°DB ì €ì¥/ë¡œë“œ) =====
def initialize_system():
    pdf_path = st.session_state.pdf_path
    vector_dir = st.session_state.vector_dir

    if not os.path.exists(pdf_path):
        st.error("âŒ 'minweonpyeonram-2025.pdf' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    if not st.session_state.index_ready:
        with st.spinner("ğŸ“„ ì¸ë±ìŠ¤ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤... (ìµœì´ˆ 1íšŒëŠ” ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆì–´ìš”)"):
            vectorstore = prepare_vectorstore(
                openai_api_key=st.session_state.api_key,
                pdf_paths=[pdf_path],
                file_names=st.session_state.file_names,
                vector_dir=vector_dir
            )
            st.session_state.retriever = build_retriever(vectorstore, k=8)
            st.session_state.index_ready = True


# ===== 5. ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ =====
def display_chat_interface():
    st.markdown(
        f"<div class='metric-card'>ğŸ“„ ë¬¸ì„œ: <b>{', '.join(st.session_state.file_names)}</b> | ğŸ’¬ ì§ˆë¬¸ ìˆ˜: {st.session_state.question_count}</div>",
        unsafe_allow_html=True
    )

    with st.expander("ì‚¬ìš© ì•ˆë‚´", expanded=False):
        st.markdown("""
        â€¢ ì‚¬ì´ë“œë°”ì—ì„œ ë¹ ë¥¸ ì§ˆë¬¸ í´ë¦­  
        â€¢ í•˜ë‹¨ ì±„íŒ…ì°½ì— ì§ì ‘ ì…ë ¥  
        â€¢ ì˜ˆì‹œ: "ì—¬ê¶Œì„ ë°œê¸‰ ë°›ê³  ì‹¶ì–´ìš”", "ë¶€ë™ì‚° ê±°ë˜ ì‹œ ì‹ ê³ ë°©ë²•ì„ ì•Œê³  ì‹¶ì–´ìš”"
        """)

    for m in st.session_state.messages:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])

    if st.session_state.selected_question and not st.session_state.processing:
        q = st.session_state.selected_question
        st.session_state.selected_question = None
        process_question_typing(q)

    if not st.session_state.processing:
        if prompt := st.chat_input("âœï¸ ë¯¼ì›ì—…ë¬´ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
            process_question_typing(prompt)


# ===== 6. íƒ€ìê¸° ìŠ¤íƒ€ì¼ ìˆœì°¨ ì¶œë ¥ + ì¶œì²˜ ê°•í™” + ë³„ì§€ì„œì‹ =====
def process_question_typing(prompt, delay=0.02):
    if st.session_state.processing:
        return
    if st.session_state.messages and st.session_state.messages[-1]["role"] == "user" \
       and st.session_state.messages[-1]["content"] == prompt:
        return

    st.session_state.processing = True
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.session_state.question_count += 1

    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        try:
            with st.spinner("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘..."):
                start_time = time.time()

                # ì»¨í…ìŠ¤íŠ¸/ì¶œì²˜/ì„œì‹ ì¶”ì¶œ
                context_text, sources_list, annex_forms = make_context_and_sources(
                    st.session_state.retriever, prompt
                )

                # ìŠ¤íŠ¸ë¦¬ë° LLM
                llm = build_streaming_llm(
                    model="gpt-4o-mini",
                    openai_api_key=st.session_state.api_key,
                    max_tokens=800,
                    temperature=0
                )

                final_prompt = build_final_prompt(
                    context=context_text,
                    question=prompt,
                    annex_forms=annex_forms
                )

                container = st.empty()
                full_text = ""
                for chunk in llm.stream(final_prompt):
                    token = getattr(chunk, "content", None)
                    if token is None:
                        token = str(chunk)
                    full_text += token
                    container.markdown(full_text)
                    time.sleep(delay)

                elapsed = round(time.time() - start_time, 2)

                # ê·¼ê±° ì¶œì²˜ ëª¨ì•„ë³´ê¸° ì„¹ì…˜ ì¶”ê°€
                if sources_list:
                    full_text += "\n\n---\nê·¼ê±° ì¶œì²˜ ëª¨ì•„ë³´ê¸°\n"
                    for i, s in enumerate(sources_list, 1):
                        full_text += f"- {i}. {s}\n"

                full_text += f"\n\n_â± {elapsed}ì´ˆ_"
                container.markdown(full_text)
                st.session_state.messages.append({"role": "assistant", "content": full_text})

        except Exception as e:
            err = f"âŒ ì˜¤ë¥˜: {e}"
            st.error(err)
            st.session_state.messages.append({"role": "assistant", "content": err})

    st.session_state.processing = False


# ===== 7. í‘¸í„° =====
def display_footer():
    st.markdown("""
    <div class="footer">
        ğŸ› ê³¡ì„±êµ°ì²­ | ğŸ“ 061-360-0000 | ğŸŒ www.gokseong.go.kr | ğŸ“ ì „ë‚¨ ê³¡ì„±êµ° ê³¡ì„±ì êµ°ì²­ë¡œ 15  
        âš  ë³¸ ì„œë¹„ìŠ¤ëŠ” AI ì•ˆë‚´ ì„œë¹„ìŠ¤ì´ë©°, ì •í™•í•œ ë¯¼ì›ì€ ë‹´ë‹¹ë¶€ì„œì— ë¬¸ì˜í•˜ì„¸ìš”.
    </div>
    """, unsafe_allow_html=True)


if __name__ == "__main__":
    main()


