import streamlit as st
import os
import time
from rag_logic import (
    prepare_vectorstore,
    build_retriever,
    build_streaming_llm,
    make_context_and_sources,
    build_final_prompt
)


def init_session_state():
    defaults = {
        "messages": [],
        "api_key": None,
        "question_count": 0,
        "processing": False,
        "selected_question": None,
        "last_clicked_question": None,
        "vector_dir": "faiss_minweonpyeonram_2025",
        "pdf_path": "minweonpyeonram-2025.pdf",
        "index_ready": False,
        "retriever": None,
        "file_names": ["ê³¡ì„±êµ° ë¯¼ì›í¸ëŒ 2025"],
        "typing_delay": 0.02,  # íƒ€ì ì†ë„
    }
    for k, v in defaults.items():
        if k not in st.session_state:
            st.session_state[k] = v


def main():
    init_session_state()
    st.set_page_config(
        page_title="ğŸ›ï¸ ê³¡ì„±êµ° AI ë¯¼ì›ìƒë‹´ë´‡",
        page_icon="ğŸ›ï¸",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    st.markdown("""
    <style>
        :root { color-scheme: dark; }
        body, .stApp { background-color: #121212; color: #fff; }
        .main-header {
            background: linear-gradient(90deg, #222 0%, #444 100%);
            padding: 1.2rem; border-radius: 10px;
            text-align: center; margin-bottom: 1rem;
        }
        .main-header h2 { margin: 0; color: #fff; }
        .main-header p { margin: 0; font-size: 0.9em; color: #bbb; }
        .metric-card {
            background: #1e1e1e; color: #eee;
            padding: 1rem; border-radius: 8px;
            border-left: 4px solid #667eea; margin-bottom: 1rem;
            font-size: 0.95em;
        }
        .footer {
            padding: 0.8rem; text-align: center;
            font-size: 0.8em; color: #aaa; border-top: 1px solid #333;
            margin-top: 1.5rem;
        }
        @media (max-width: 768px) {
            .main-header h2 { font-size: 1.2em; }
            .main-header p { font-size: 0.8em; }
            .metric-card { font-size: 0.85em; padding: 0.8rem; }
            .footer { font-size: 0.7em; padding: 0.5rem; }
        }
    </style>
    """, unsafe_allow_html=True)

    st.markdown("""
    <div class="main-header">
        <h2>ğŸ›ï¸ ê³¡ì„±êµ° AI ë¯¼ì›ìƒë‹´ë´‡</h2>
        <p>ê³¡ì„±êµ° ë¯¼ì›í¸ëŒ ê¸°ë°˜ AI ìƒë‹´ ì„œë¹„ìŠ¤</p>
    </div>
    """, unsafe_allow_html=True)

    setup_sidebar()

    if not st.session_state.api_key:
        st.warning("ğŸ”‘ ì‚¬ì´ë“œë°”ì—ì„œ OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.stop()

    initialize_system()
    display_chat_interface()
    display_footer()


def setup_sidebar():
    st.sidebar.title("API ì„¤ì •")
    key = st.sidebar.text_input("OpenAI API í‚¤", type="password", key="api_key_input")
    if key:
        st.session_state.api_key = key
   
    st.sidebar.markdown("---")
    st.sidebar.subheader("ë¹ ë¥¸ ì§ˆë¬¸")
    quick_qs = [
        "ì—¬ê¶Œì„ ë°œê¸‰ ë°›ê³  ì‹¶ì–´ìš”",
        "ì „ì…ì‹ ê³  ë°©ë²•ì„ ì•Œê³  ì‹¶ì–´ìš”",
        "ì¸ê°ì¦ëª…ì„œ ë°œê¸‰ ë°›ê³  ì‹¶ì–´ìš”",
        "ì •ë³´ê³µê°œë¥¼ ì²­êµ¬ë°©ë²•ì„ ì•Œê³  ì‹¶ì–´ìš”",
        "ê±´ì¶•í—ˆê°€ ì‹ ì²­ ì ˆì°¨ë¥¼ ì•Œê³  ì‹¶ì–´ìš”"
    ]
    for q in quick_qs:
        if st.sidebar.button(q, key=f"btn_{q}"):
            if not st.session_state.processing and st.session_state.last_clicked_question != q:
                st.session_state.selected_question = q
                st.session_state.last_clicked_question = q

    st.sidebar.markdown("---")
    if st.sidebar.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.messages.clear()
        st.session_state.question_count = 0
        st.session_state.selected_question = None
        st.session_state.last_clicked_question = None
        st.experimental_rerun()


def initialize_system():
    pdf_path = st.session_state.pdf_path
    vector_dir = st.session_state.vector_dir

    if not os.path.exists(pdf_path):
        st.error(f"âŒ '{pdf_path}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()

    if not st.session_state.index_ready:
        with st.spinner("ğŸ“„ ì¸ë±ìŠ¤ ì¤€ë¹„ ì¤‘..."):
            vectorstore = prepare_vectorstore(
                openai_api_key=st.session_state.api_key,
                pdf_paths=[pdf_path],
                file_names=st.session_state.file_names,
                vector_dir=vector_dir
            )
            st.session_state.retriever = build_retriever(vectorstore, k=8)
            st.session_state.index_ready = True


def display_chat_interface():
    st.markdown(
        f"<div class='metric-card'>ğŸ“„ ë¬¸ì„œ: <b>{', '.join(st.session_state.file_names)}</b> | ğŸ’¬ ì§ˆë¬¸ ìˆ˜: {st.session_state.question_count}</div>",
        unsafe_allow_html=True
    )

    for m in st.session_state.messages:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])

    if st.session_state.selected_question and not st.session_state.processing:
        q = st.session_state.selected_question
        st.session_state.selected_question = None
        process_question_typing(q, st.session_state.typing_delay)

    if not st.session_state.processing:
        if prompt := st.chat_input("âœï¸ ê¶ê¸ˆí•œ ë¯¼ì›ì„ ì…ë ¥í•˜ì„¸ìš”..."):
            process_question_typing(prompt, st.session_state.typing_delay)


def process_question_typing(prompt, delay=0.02):
    if st.session_state.processing:
        return
    if st.session_state.messages and st.session_state.messages[-1]["role"] == "user" and \
       st.session_state.messages[-1]["content"] == prompt:
        return

    st.session_state.processing = True
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.session_state.question_count += 1

    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        try:
            container = st.empty()
            with st.spinner("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘..."):
                context_text, _, annex_forms = make_context_and_sources(
                    st.session_state.retriever, prompt
                )

                llm = build_streaming_llm(
                    model="gpt-4o-mini",
                    openai_api_key=st.session_state.api_key,
                    max_tokens=800,
                    temperature=0
                )

                final_prompt = build_final_prompt(
                    context=context_text,
                    question=prompt,
                    annex_forms=annex_forms
                )

                full_text = ""
                for chunk in llm.stream(final_prompt):
                    # ë©”íƒ€ë°ì´í„°ê°€ ì¶œë ¥ë˜ì§€ ì•Šë„ë¡ contentë§Œ ì•ˆì „í•˜ê²Œ ì‚¬ìš©
                    token = getattr(chunk, "content", None)
                    if not token:
                        continue
                    full_text += token
                    container.markdown(full_text)
                    time.sleep(delay)

                # ì²˜ë¦¬ì‹œê°„/ê·¼ê±° ì¶œì²˜ ëª¨ì•„ë³´ê¸° ì¶œë ¥ ì œê±°: ì•„ë¬´ ê²ƒë„ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
                st.session_state.messages.append({"role": "assistant", "content": full_text})

        except Exception as e:
            err_msg = f"âŒ ì˜¤ë¥˜: {e}"
            st.error(err_msg)
            st.session_state.messages.append({"role": "assistant", "content": err_msg})

    st.session_state.processing = False


def display_footer():
    st.markdown("""
    <div class="footer">
        ğŸ› ê³¡ì„±êµ°ì²­ | ğŸ“ 061-360-0000 | ğŸŒ www.gokseong.go.kr | ğŸ“ ì „ë‚¨ ê³¡ì„±êµ° ê³¡ì„±ì êµ°ì²­ë¡œ 15  
        âš  ë³¸ ì„œë¹„ìŠ¤ëŠ” AI ì•ˆë‚´ ì„œë¹„ìŠ¤ì´ë©°, ì •í™•í•œ ë¯¼ì›ì€ ë‹´ë‹¹ë¶€ì„œì— ë¬¸ì˜í•˜ì„¸ìš”.
    </div>
    """, unsafe_allow_html=True)


if __name__ == "__main__":
    main()







